


# LLM Spot v0.1.0-Pre-aplha


## Installation

### Requirements
- Node.js
- npm (or another package manager)
- Python 3
- Pipenv

### Steps
1. Clone the repository.
2. Navigate to `./backend` and run:
   ```bash
   pipenv install
   python ./api.py
   ```
3. In the root directory, start the frontend:
	```bash
	npm start
	```

## Development Status

### Implemented Features
- [x] Core chat functionality
- [x] Basic API integration
- [x] Markdown support
- [x] File attachment support
- [x] Conversation context

### Planned Features
- [ ] Parsing Code Blocks into Code HTML Element.
- [ ] Setting personal tokens by user
- [ ] Splitting chatting into conversations
- [ ] More keyboard shortcuts
- [ ] Processing image requests by LLM
- [ ] Support of multiple LLMs
- [ ] Better website responsiveness
- [ ] Error handling
- [ ] Basic documentation
- [ ] Installation guide
- [ ] Basic test coverage

### Known Issues
- Single conversation per browser session
- Conversation history lost on refresh
- Limited responsiveness on medium/small screens
- Code Blocks generated by an LLM are not parsed properly.
- Panel Action Bar doesnt have minimal height.

# LLM Spot v0.2.0-Pre-alpha

## Installation

> Installation steps and dependencies remain same as for v0.1.0-Pre-aplha.

## Development Status

### Implemented Features

- Core chat functionality.
- Basic API integration.
- Markdown support.
- File attachment support.
- Conversation context management.
- Parsing and highlighting of code blocks.
- Support for multiple conversations per session.
- Persistent conversation history.

### Planned Features

- User-configurable tokens.
- Enhanced keyboard shortcuts.
- Image processing by LLM.
- Support for multiple LLMs.
- Improved responsiveness and UX.
- Comprehensive error handling.
- Documentation and installation guide.
- Basic test coverage.
### Changelog

#### Added

- Support for multiple conversations per session.
- Persistent conversation history stored on the server.
- Improved code block parsing and highlighting.

#### Improved

- Website responsiveness on smaller screens.
- Collapsible tuning parameters container.

#### Fixed

- Panel action bar now has a minimum height.

### Known Issues

- Responsiveness can still be improved for very small screens.
- UX enhancements, such as better status indicators for requests.
- Font in parsed code blocks is too bold.
- Font in messages seems rather big.
- Account settings remain non-functional.
- LLM responses should be processed in stream.
- Web App still supports only one model.
- OpenAI's models not loaded properly after OpenAI service is selected.
- At the start, additional parameters aren't collapsed properly.

# LLM Spot v0.3.0-Pre-alpha

## Development Status

### Implemented Features

- Core chat functionality.
- Basic API integration.
- Markdown support.
- File attachment support.
- Conversation context management.
- Parsing and highlighting of code blocks.
- Support for multiple conversations per session.
- Persistent conversation history.
- App Containerization.
- LLM Responses returned as streams.


### Planned Features
- Integrating Anthropic and Google into supported services.
- Adding more UX design by inserting loading animation to imply Client-Server communication.
- Move ConversationList from standard layout into upper layout.

### Changelog

#### Added

- App servers are now containerized in Docker Containers.

#### Improved

- LLM's reponses are returned, processed and displayed in streams.
- Supported services are now loaded dynamically.


#### Fixed

- Additional Parameters are collapsed properly.
- OpenAI's models are now loaded when selecting this service.

### Known Issues